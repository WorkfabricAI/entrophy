{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "ENTROPHY Â© 2025 by Workfabric  \n",
    "Licensed under Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International  \n",
    "https://creativecommons.org/licenses/by-nc-sa/4.0/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Get Embeddings\n",
    "\n",
    "This section will compute the embeddings for the process instances.\n",
    "\n",
    "We provide the embeddings in CSV files for each domain, if you simply want to reproduce the figures, please go to Section 2 below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tiktoken\n",
    "from openai import OpenAI\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8000  # the maximum for text-embedding-3-small is 8191\n",
    "API_KEY = \"API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_JSON = \"\"\n",
    "df = pd.read_json(PATH_TO_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = (\n",
    "    df.description\n",
    "    + \" in the screen \"\n",
    "    + df.screen_name\n",
    "    + \" of the application \"\n",
    "    + df.application_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = tiktoken.get_encoding(embedding_encoding)\n",
    "client = OpenAI(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "    # Check if text is too large for the model's token limit\n",
    "    tokens = encoder.encode(text)\n",
    "    if len(tokens) <= max_tokens:\n",
    "        # If text fits within token limit, get embedding directly\n",
    "        return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "    else:\n",
    "        # If text is too large, break it into chunks and average the embeddings\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        # Split into chunks that fit within the token limit\n",
    "        for token in tokens:\n",
    "            if current_length + 1 > max_tokens:\n",
    "                # Current chunk is full, add it to chunks and start a new one\n",
    "                chunks.append(encoder.decode(current_chunk))\n",
    "                current_chunk = [token]\n",
    "                current_length = 1\n",
    "            else:\n",
    "                # Add token to current chunk\n",
    "                current_chunk.append(token)\n",
    "                current_length += 1\n",
    "\n",
    "        # Add the last chunk if it's not empty\n",
    "        if current_chunk:\n",
    "            chunks.append(encoder.decode(current_chunk))\n",
    "\n",
    "        # Get embeddings for each chunk\n",
    "        chunk_embeddings = []\n",
    "        for chunk in chunks:\n",
    "            embedding = (\n",
    "                client.embeddings.create(input=[chunk], model=model).data[0].embedding\n",
    "            )\n",
    "            chunk_embeddings.append(embedding)\n",
    "\n",
    "        # Average the embeddings\n",
    "        avg_embedding = np.mean(chunk_embeddings, axis=0).tolist()\n",
    "\n",
    "        return avg_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with process information\n",
    "process_instances = []\n",
    "for process_instance_uuid, group in df.groupby(\"process_instance_uuid\"):\n",
    "    process_name = group[\"process_name\"].iloc[\n",
    "        0\n",
    "    ]  # Get process name from first row in group\n",
    "    process_instance_text = \"\\n\".join(group.text.tolist())\n",
    "    process_instances.append(\n",
    "        {\n",
    "            \"process_instance_uuid\": process_instance_uuid,\n",
    "            \"process_name\": process_name,\n",
    "            \"process_instance_text\": process_instance_text,\n",
    "        }\n",
    "    )\n",
    "\n",
    "process_df = pd.DataFrame(process_instances)\n",
    "process_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_df[\"num_tokens\"] = process_df.process_instance_text.apply(\n",
    "    lambda x: len(encoder.encode(x))\n",
    ")\n",
    "process_df[\"embedding\"] = process_df.process_instance_text.apply(\n",
    "    lambda x: get_embedding(x, model=\"text-embedding-3-small\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the process dataframe to CSV\n",
    "# process_df.to_csv(f'CSV_PATH', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Generate Plots\n",
    "\n",
    "This section uses the provided CSV files, which contains the embeddings for all process instances.\n",
    "\n",
    "Please execute the cell below to produce the heatmaps of similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "domains = [\"hr\", \"legal\", \"finance\"]\n",
    "fig, axes = plt.subplots(3, 1, figsize=(8, 16))  # Adjusted for potentially wider labels\n",
    "\n",
    "for i, domain in enumerate(domains):\n",
    "    # Load embeddings\n",
    "    embeddings_path = f\"{domain}_embeddings.csv\"\n",
    "    embeddings_df = pd.read_csv(embeddings_path)\n",
    "    process_names = embeddings_df[\"process_name\"].tolist()\n",
    "\n",
    "    # compute similarity matrix\n",
    "    embeddings_array = np.array(\n",
    "        embeddings_df.embedding.apply(lambda x: np.array(eval(x))).tolist()\n",
    "    )\n",
    "    similarity_matrix = cosine_similarity(embeddings_array)\n",
    "\n",
    "    # Create a DataFrame for Seaborn's heatmap\n",
    "    similarity_df = pd.DataFrame(\n",
    "        similarity_matrix, index=process_names, columns=process_names\n",
    "    )\n",
    "\n",
    "    similarity_df_cleaned = similarity_df.dropna(axis=0, how=\"all\").dropna(\n",
    "        axis=1, how=\"all\"\n",
    "    )\n",
    "\n",
    "    # Sort the DataFrame by process name\n",
    "    similarity_df_sorted = similarity_df_cleaned.sort_index().sort_index(axis=1)\n",
    "\n",
    "    # Get unique process names to create consolidated labels\n",
    "    unique_processes = []\n",
    "    unique_indices = []\n",
    "\n",
    "    # Find the first occurrence of each unique process name\n",
    "    for idx, process in enumerate(similarity_df_sorted.index):\n",
    "        if len(process) > 20:\n",
    "            process = process[:20] + \"...\"\n",
    "        if process not in unique_processes:\n",
    "            unique_processes.append(process)\n",
    "            unique_indices.append(idx)\n",
    "\n",
    "    # Plot the heatmap on the i-th subplot with increased font sizes\n",
    "    g = sns.heatmap(\n",
    "        similarity_df_sorted,\n",
    "        ax=axes[i],\n",
    "        cmap=\"PiYG\",\n",
    "        center=0.75,\n",
    "        linewidths=0.4,\n",
    "        cbar_kws={\"label\": \"Cosine Similarity\"},\n",
    "    )\n",
    "\n",
    "    if domain == \"hr\":\n",
    "        axes[i].set_title(\"HR\", fontsize=14, fontweight=\"bold\", pad=20)\n",
    "    else:\n",
    "        axes[i].set_title(\n",
    "            f\"{domain.capitalize()}\", fontsize=14, fontweight=\"bold\", pad=20\n",
    "        )\n",
    "    axes[i].set_xlabel(\"Workflow Instances\", fontsize=12, labelpad=10)\n",
    "    axes[i].set_ylabel(\"Workflow Instances\", fontsize=12, labelpad=10)\n",
    "\n",
    "    # Hide all tick labels first\n",
    "    axes[i].set_xticks([])\n",
    "    axes[i].set_yticks([])\n",
    "\n",
    "    # Create a mapping of process names to indices\n",
    "    process_indices = {}\n",
    "    for j, process in enumerate(unique_processes):\n",
    "        process_indices[process] = j\n",
    "\n",
    "    # Add process labels as annotations instead of tick labels\n",
    "    # Create a separate legend/key figure for process names\n",
    "    process_colors = plt.cm.tab20(np.linspace(0, 1, len(unique_processes)))\n",
    "\n",
    "    # Add colored blocks at the edges of the heatmap\n",
    "    for j in range(len(unique_indices)):\n",
    "        start_idx = unique_indices[j]\n",
    "        end_idx = (\n",
    "            unique_indices[j + 1]\n",
    "            if j < len(unique_indices) - 1\n",
    "            else len(similarity_df_sorted)\n",
    "        )\n",
    "        width = end_idx - start_idx\n",
    "\n",
    "        # Add colored rectangles at the top and left edges\n",
    "        rect_color = process_colors[j]\n",
    "\n",
    "        # Top edge - make thicker\n",
    "        axes[i].add_patch(\n",
    "            plt.Rectangle(\n",
    "                (start_idx, -1.0),\n",
    "                width,\n",
    "                1.0,\n",
    "                fill=True,\n",
    "                color=rect_color,\n",
    "                clip_on=False,\n",
    "            )\n",
    "        )\n",
    "        # Left edge - make thicker\n",
    "        axes[i].add_patch(\n",
    "            plt.Rectangle(\n",
    "                (-1.0, start_idx),\n",
    "                1.0,\n",
    "                width,\n",
    "                fill=True,\n",
    "                color=rect_color,\n",
    "                clip_on=False,\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Add a legend below each heatmap with larger font size\n",
    "    legend_elements = [\n",
    "        plt.Rectangle((0, 0), 1, 1, color=process_colors[j])\n",
    "        for j in range(len(unique_processes))\n",
    "    ]\n",
    "\n",
    "    # Create a more readable legend to the right of the heatmap\n",
    "    axes[i].legend(\n",
    "        legend_elements,\n",
    "        unique_processes,\n",
    "        loc=\"center left\",\n",
    "        bbox_to_anchor=(1.25, 0.5),\n",
    "        fontsize=10,\n",
    "        frameon=False,\n",
    "        ncol=1,\n",
    "        title=\"Process Names\",\n",
    "        title_fontsize=12,\n",
    "    )\n",
    "\n",
    "    # Add thicker lines to separate different processes\n",
    "    for idx in unique_indices[1:]:  # Skip the first index\n",
    "        # Add horizontal and vertical lines\n",
    "        axes[i].axhline(y=idx, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "        axes[i].axvline(x=idx, color=\"black\", linestyle=\"-\", linewidth=1)\n",
    "\n",
    "plt.tight_layout(\n",
    "    rect=[0, 0.05, 1, 0.96]\n",
    ")  # Adjust layout for suptitle and x-axis labels\n",
    "\n",
    "# Save the figure to PDF\n",
    "plt.savefig(\"process_similarity_heatmap.pdf\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "entrophy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
